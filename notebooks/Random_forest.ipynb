{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9ee1af",
   "metadata": {},
   "source": [
    "RANDOM FOREST !!!\n",
    "\n",
    "Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. Instead of relying on a single decision tree (which can be sensitive to noise or overfit the training data), random forests create many trees during training, each using a different random subset of the data and features — a technique known as bagging (Bootstrap Aggregating). For classification tasks, each tree votes for a class label, and the majority vote becomes the final prediction. For regression, the final output is the average of all tree predictions. Random forests are robust to noise, handle missing data well, and perform well even without feature scaling. Because of the randomness in data and feature selection, they generalize better than individual decision trees and often achieve higher accuracy in real-world tasks.\n",
    "\n",
    "For the scratch implementation, we are going to use the same code from decision trees. But we will built Random forest classifier on top of it.\n",
    "\n",
    "Let's Gooo...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9996fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 83.54 %\n",
      "Test Accuracy: 76.92 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    classes = np.unique(y)\n",
    "    impurity = 1.0\n",
    "    for cls in classes:\n",
    "        p = np.sum(y == cls) / len(y)\n",
    "        impurity -= p ** 2\n",
    "    return impurity\n",
    "\n",
    "def best_split(X, y):\n",
    "    best_feature, best_threshold, best_gini = None, None, 1.0\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    for feature in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            left = y[X[:, feature] <= threshold]\n",
    "            right = y[X[:, feature] > threshold]\n",
    "\n",
    "            if len(right) == 0 or len(left) ==0:\n",
    "                continue\n",
    "\n",
    "            gini_left = gini(left)\n",
    "            gini_right = gini(right)\n",
    "            weighted_gini = (len(right) * gini(right) + len(left) * gini(left)) / len(y)\n",
    "\n",
    "            if weighted_gini < best_gini:\n",
    "             best_gini = weighted_gini\n",
    "             best_feature = feature\n",
    "             best_threshold = threshold\n",
    "\n",
    "    return best_feature , best_threshold\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "def build_tree(X,y, depth = 0, max_depth = 5):\n",
    "    if len(set(y)) == 1 or depth >= max_depth:\n",
    "        most_common = np.bincount(y).argmax()\n",
    "        return Node(value = most_common)\n",
    "\n",
    "    feature, threshold = best_split(X, y)\n",
    "    if feature is None:\n",
    "        most_common = np.bincount(y).argmax()\n",
    "        return Node(value = most_common)\n",
    "\n",
    "    left_indices = X[:, feature] <= threshold\n",
    "    right_indices = X[:,feature] > threshold\n",
    "\n",
    "    left_subtree = build_tree(X[left_indices], y[left_indices], depth + 1, max_depth = 3)\n",
    "    right_subtree = build_tree(X[right_indices], y[right_indices], depth + 1, max_depth = 3)\n",
    "\n",
    "    return Node(feature, threshold, left_subtree, right_subtree)\n",
    "\n",
    "def predict(sample, tree):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "    if sample[tree.feature] <= tree.threshold:\n",
    "        return predict(sample, tree.left)\n",
    "    else:\n",
    "        return predict(sample, tree.right)\n",
    "\n",
    "def predict_all(X, tree):\n",
    "    return np.array([predict(sample, tree) for sample in X])\n",
    "\n",
    "class RandomForestClassifierScratch:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sampling (with replacement)\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            tree = build_tree(X_sample, y_sample, max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get predictions from each tree\n",
    "        tree_preds = np.array([predict_all(X, tree) for tree in self.trees])\n",
    "        # Majority vote\n",
    "        majority_votes = np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=tree_preds)\n",
    "        return majority_votes\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/Titanic-Dataset.csv\")              \n",
    "\n",
    "'''I kept the same code, just added the Random forest classifier and changed the dataset from Iris to Titanic'''\n",
    "\n",
    "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch']].dropna()\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "X = df[['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch']]  \n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifierScratch(n_estimators=100, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train_preds = rf.predict(X_train)\n",
    "test_preds = rf.predict(X_test)\n",
    "\n",
    "train_acc = np.mean(train_preds == y_train)\n",
    "test_acc = np.mean(test_preds == y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", round(train_acc * 100, 2), \"%\")\n",
    "print(\"Test Accuracy:\", round(test_acc * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6fd70",
   "metadata": {},
   "source": [
    "In this analysis, both the Decision Tree and Random Forest classifiers were applied to the Titanic dataset. The Decision Tree achieved a training accuracy of approximately 81.37% and a test accuracy of 76.22%, while the Random Forest slightly improved these figures to 83.54% and 76.92% respectively. Although the Random Forest combines multiple decision trees to reduce overfitting and improve generalization, the marginal improvement here suggests that the dataset’s size and complexity are not sufficient to highlight the full strength of ensemble methods. Additionally, the features used may not provide enough variability or richness for the Random Forest to significantly outperform a single tree. Still, the Random Forest shows better robustness and slightly enhanced predictive performance, which supports its use in more complex or noisy real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d1d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MNIST dataset...\n",
      "Training Random Forest...\n",
      "\n",
      " Train Accuracy: 99.94%\n",
      " Test Accuracy:  96.52%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1343\n",
      "           1       0.98      0.98      0.98      1600\n",
      "           2       0.95      0.97      0.96      1380\n",
      "           3       0.96      0.95      0.95      1433\n",
      "           4       0.96      0.96      0.96      1295\n",
      "           5       0.97      0.96      0.96      1273\n",
      "           6       0.98      0.98      0.98      1396\n",
      "           7       0.97      0.96      0.97      1503\n",
      "           8       0.95      0.95      0.95      1357\n",
      "           9       0.94      0.95      0.95      1420\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.96      0.96     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"Downloading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n Train Accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\" Test Accuracy:  {test_acc * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00013a3",
   "metadata": {},
   "source": [
    "The Random Forest algorithm showed a massive performance improvement on the MNIST dataset compared to the Titanic dataset. This modest improvement over a single decision tree is due to the limited and noisy nature of the Titanic features. In contrast, on the MNIST dataset—a large, clean, and high-dimensional image dataset of handwritten digits—Random Forest achieved an impressive 99.94% training accuracy and 96.52% test accuracy. This dramatic leap is because Random Forest thrives in environments with complex patterns and abundant features. The large size of MNIST allows each tree in the forest to learn distinct visual patterns, and the ensemble effectively generalizes across the dataset. This comparison highlights that while Random Forest can offer stability and slight gains on small, simple datasets, its true strength emerges with rich, high-dimensional data where individual decision trees capture diverse aspects of the problem space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c100c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
